{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juCcnLMQuPwp"
   },
   "source": [
    "Script to\n",
    "1. Propagate Relationships, add missing Authority Terms to Relationship entries, and return csv for Standardized Directory\n",
    "2. Return csv according to wikidata person schema for upload to OpenRefine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1Ozs7jt1yDU"
   },
   "source": [
    "Plan:\n",
    "1. import curated relationships and persons lists\n",
    "2. dedup standard names from df_persons;\n",
    "3. add standard names to df_relationships based on a) matching; b) missing values (using first, last names, etc)\n",
    "4. propagate df_relationships\n",
    "  4a. approach: row-wise: for every row, look at the kind of relationship; produce an inverted copy of it; seek third-party relationships and produce those (see 4b); deduplicate relationships\n",
    "  4b. look at current row relationship (p1 to p2); loop through p2 relationships (with p3); assert p1 -> p3 relationships; loop twice, then deduplicate rows?\n",
    "  4c. example: if current row has p1 as husband to p2, then script should find all relationships for p2; if p2 is mother to p3 and sister to p4, assert p1 is father to p3 (is that risky?) and p1 is brother-in-law to p4.\n",
    "  4d. need a list of all\n",
    "5. build df that matches person schema for wikidata, combining df_persons and df_relationships and output as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C0nlb7S4fgTZ"
   },
   "outputs": [],
   "source": [
    "# import requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nkog0M3Df3OC"
   },
   "outputs": [],
   "source": [
    "# Load the files from the current folder\n",
    "df = pd.read_excel('./data/relationships_2_28_25.xlsx')\n",
    "df_relationships = df.replace('', np.nan, regex=True)\n",
    "df_2 = pd.read_excel('./data/standard_persons_4_21_25.xlsx')\n",
    "df_persons = df_2.replace('', np.nan, regex=True).sample(n=50, random_state=42)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NC7D-2BEvrOe",
    "outputId": "fb807beb-587d-4e33-901a-ae5127656dba"
   },
   "outputs": [],
   "source": [
    "df_relationships.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Bo_K9KNKyiG"
   },
   "outputs": [],
   "source": [
    "df_relationships.drop(['Column17', 'Read Me', 'Column15', 'Column16'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBmL86v9PiI_",
    "outputId": "bd1dfda6-bd6e-4698-c7cb-eee5b7e0eb03"
   },
   "outputs": [],
   "source": [
    "df_persons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u9-WaK_m1_G3"
   },
   "outputs": [],
   "source": [
    "df_persons.drop(['Mention only?', 'Researcher/Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "8B6ixtPw-EPB",
    "outputId": "863af100-8f74-48ee-9670-4de37e698fb7"
   },
   "outputs": [],
   "source": [
    "df_persons[df_persons['Authority Terms']=='Audland, Anne'][['Birth Date', 'Death Date', 'Marriage Date']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxMquhZ2HBQo"
   },
   "source": [
    "0. data cleaning tasks;\n",
    "1. dedup standard names from df_persons;\n",
    "2. add standard names to df_relationships based on a) matching; b) missing values (using first, last names, etc)\n",
    "3. propagate df_relationships\n",
    "4. build df that matches person schema for wikidata, combining df_persons and df_relationships and output as .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPwyVO7iuOAf"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkoE_3l_WmDS"
   },
   "source": [
    "#Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "dwkTHoWVf9Z4"
   },
   "outputs": [],
   "source": [
    "from pickle import FALSE\n",
    "#delete empty rows\n",
    "df_relationships.dropna(how='all', inplace=True)\n",
    "df_persons.dropna(how='all', inplace=True)\n",
    "\n",
    "#delete any row in def_persons that doesn't have an Authority Name:\n",
    "df_persons = df_persons[df_persons['Authority Terms'].notna()]\n",
    "\n",
    "#delete any authority terms that have a bracket\n",
    "df_persons = df_persons[~df_persons['Authority Terms'].str.contains(r'\\[|\\]|\\(|\\)')]\n",
    "#df_relationships.info()\n",
    "#df_persons.info()\n",
    "\n",
    "#rename column\n",
    "df_persons.rename(columns={\"Authority Terms\": \"name\"}, inplace=True)\n",
    "#CHANGE THIS!!! WRONG -- NEED PERSOn 1 Standard... and Person 2 standard to be converted\n",
    "df_relationships.rename(columns={\"Person 1 Standard name from List\": \"p1_std_name\", 'Person 2 Standard name from List': 'p2_std_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ruwn3s5Qdoa9",
    "outputId": "6b008a13-e9a7-47e7-e036-02c8116f1f8e"
   },
   "outputs": [],
   "source": [
    "df_relationships.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dcKu0Fy83LzV"
   },
   "outputs": [],
   "source": [
    "# Function to process date columns for both handlebars and brackets\n",
    "def process_date_columns(row):\n",
    "    # Define the columns to process and their corresponding target columns for handlebars\n",
    "    handlebars_columns = {\n",
    "        'Birth Date': 'DoB Source URL',\n",
    "        'Death Date': 'DoD Source URL',\n",
    "        'Marriage Date': 'P26+P2562 Source URL'\n",
    "    }\n",
    "\n",
    "    # Define the columns to process and their corresponding target columns for brackets\n",
    "    brackets_columns = {\n",
    "        'Birth Date': 'DoB Source',\n",
    "        'Death Date': 'DoD (P570) Source',\n",
    "        'Marriage Date': 'P26+P2562 Source'\n",
    "    }\n",
    "\n",
    "    # Process handlebars ({{ }})\n",
    "    for col, target_col in handlebars_columns.items():\n",
    "        if pd.notna(row[col]) and isinstance(row[col], str):  # Check if the column value is not NaN and is a string\n",
    "            # Extract text between double handlebars\n",
    "            matches = re.findall(r'\\{\\{(.*?)\\}\\}', row[col])\n",
    "            if matches:\n",
    "                # Remove text between double handlebars and the handlebars themselves\n",
    "                row[col] = re.sub(r'\\{\\{.*?\\}\\}', '', row[col]).strip()\n",
    "\n",
    "                # Prepare the source text\n",
    "                source_text = '; '.join(matches)\n",
    "\n",
    "                # Append to the target column with a semi-colon separator if it already has data\n",
    "                if pd.notna(row[target_col]) and row[target_col].strip():\n",
    "                    row[target_col] += '; ' + source_text\n",
    "                else:\n",
    "                    row[target_col] = source_text\n",
    "\n",
    "    # Process brackets ([[ ]])\n",
    "    for col, target_col in brackets_columns.items():\n",
    "        if pd.notna(row[col]) and isinstance(row[col], str):  # Check if the column value is not NaN and is a string\n",
    "            # Extract text between double brackets\n",
    "            matches = re.findall(r'\\[\\[(.*?)\\]\\]', row[col])\n",
    "            if matches:\n",
    "                # Remove text between double brackets and the brackets themselves\n",
    "                row[col] = re.sub(r'\\[\\[.*?\\]\\]', '', row[col]).strip()\n",
    "\n",
    "                # Prepare the source text\n",
    "                source_text = '; '.join(matches)\n",
    "\n",
    "                # Append to the target column with a semi-colon separator if it already has data\n",
    "                if pd.notna(row[target_col]) and row[target_col].strip():\n",
    "                    row[target_col] += '; ' + source_text\n",
    "                else:\n",
    "                    row[target_col] = source_text\n",
    "    return row\n",
    "\n",
    "# Function to strip leading and trailing whitespace from specific date and source columns\n",
    "def strip_whitespace_from_specific_columns(df):\n",
    "    # Define the specific columns to clean\n",
    "    columns_to_clean = [\n",
    "        'Birth Date', 'Death Date', 'Marriage Date',\n",
    "        'DoB Source', 'DoD (P570) Source', 'P26+P2562 Source',\n",
    "        'DoB Source URL', 'DoD Source URL', 'P26+P2562 Source URL'\n",
    "    ]\n",
    "\n",
    "    # Strip leading and trailing whitespace from these columns\n",
    "    for col in columns_to_clean:\n",
    "        if col in df.columns:  # Ensure the column exists in the DataFrame\n",
    "            df[col] = df[col].astype(str).str.strip()  # Convert to string and strip whitespace\n",
    "    return df\n",
    "\n",
    "# Add new columns to the DataFrame for both handlebars and bracket-based sources\n",
    "df_persons['DoB Source'] = ''\n",
    "df_persons['DoD (P570) Source'] = ''\n",
    "df_persons['P26+P2562 Source'] = ''\n",
    "df_persons['DoB Source URL'] = ''\n",
    "df_persons['DoD Source URL'] = ''\n",
    "df_persons['P26+P2562 Source URL'] = ''\n",
    "\n",
    "# Apply the functions to the DataFrame\n",
    "df_persons = df_persons.apply(process_date_columns, axis=1)\n",
    "df_persons = strip_whitespace_from_specific_columns(df_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "Bi7c8QPxSKuM",
    "outputId": "466ed0a2-8ea7-46eb-f5a9-3bd4c6ff09cf"
   },
   "outputs": [],
   "source": [
    "df_persons[df_persons['name']=='Audland, Anne'][['Birth Date', 'Death Date', 'Marriage Date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5zbmbi7BbTVg"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Identify all date columns in df_persons\n",
    "date_columns = ['Birth Date', 'Death Date', 'Marriage Date']\n",
    "\n",
    "# Function to handle partial dates\n",
    "def fix_partial_date(date_str):\n",
    "    if pd.isna(date_str) or date_str.strip() == '':\n",
    "        return None  # Handle missing or empty values\n",
    "    date_str = date_str.strip()\n",
    "    try:\n",
    "        if date_str.endswith('-00-00'):  # Year only (e.g., '1627-00-00')\n",
    "            year = int(date_str[:4])\n",
    "            return datetime(year, 1, 1)  # Represent the year as January 1st\n",
    "        elif date_str.endswith('-00'):  # Year and month only (e.g., '1627-01-00')\n",
    "            year, month = map(int, date_str.split('-')[:2])\n",
    "            return datetime(year, month, 1)  # Represent the month as the 1st day\n",
    "        else:  # Full date\n",
    "            return pd.to_datetime(date_str, errors='coerce')\n",
    "    except Exception:\n",
    "        return None  # Return None if conversion fails\n",
    "\n",
    "# Apply the function to each date column\n",
    "for col in date_columns:\n",
    "    df_persons[col] = df_persons[col].astype(str).apply(\n",
    "        lambda x: fix_partial_date(x) if pd.notna(fix_partial_date(x)) else x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SaV22YxNzyqH"
   },
   "outputs": [],
   "source": [
    "# Function to process place columns for both handlebars and brackets\n",
    "def process_place_columns(row):\n",
    "    # Define the columns to process and their corresponding target columns for handlebars\n",
    "    handlebars_columns = {\n",
    "        'Place of Birth (P19)': 'PoB Source URL',\n",
    "        'Place of Death': 'Place of Death Source URL',\n",
    "        'Place of Residence': 'Place of Residence Source URL'\n",
    "    }\n",
    "\n",
    "    # Define the columns to process and their corresponding target columns for brackets\n",
    "    brackets_columns = {\n",
    "        'Place of Birth (P19)': 'PoB Source',\n",
    "        'Place of Death': 'Place of Death Source',\n",
    "        'Place of Residence': 'Place of Residence Source'\n",
    "    }\n",
    "\n",
    "    # Process handlebars ({{ }})\n",
    "    for col, target_col in handlebars_columns.items():\n",
    "        if pd.notna(row[col]) and isinstance(row[col], str):  # Check if the column value is not NaN and is a string\n",
    "            # Extract text between double handlebars\n",
    "            matches = re.findall(r'\\{\\{(.*?)\\}\\}', row[col])\n",
    "            if matches:\n",
    "                # Remove text between double handlebars and the handlebars themselves\n",
    "                row[col] = re.sub(r'\\{\\{.*?\\}\\}', '', row[col]).strip()\n",
    "\n",
    "                # Prepare the source text\n",
    "                source_text = '; '.join(matches)\n",
    "\n",
    "                # Append to the target column with a semi-colon separator if it already has data\n",
    "                if pd.notna(row[target_col]) and row[target_col].strip():\n",
    "                    row[target_col] += '; ' + source_text\n",
    "                else:\n",
    "                    row[target_col] = source_text\n",
    "\n",
    "    # Process brackets ([[ ]])\n",
    "    for col, target_col in brackets_columns.items():\n",
    "        if pd.notna(row[col]) and isinstance(row[col], str):  # Check if the column value is not NaN and is a string\n",
    "            # Extract text between double brackets\n",
    "            matches = re.findall(r'\\[\\[(.*?)\\]\\]', row[col])\n",
    "            if matches:\n",
    "                # Remove text between double brackets and the brackets themselves\n",
    "                row[col] = re.sub(r'\\[\\[.*?\\]\\]', '', row[col]).strip()\n",
    "\n",
    "                # Prepare the source text\n",
    "                source_text = '; '.join(matches)\n",
    "\n",
    "                # Append to the target column with a semi-colon separator if it already has data\n",
    "                if pd.notna(row[target_col]) and row[target_col].strip():\n",
    "                    row[target_col] += '; ' + source_text\n",
    "                else:\n",
    "                    row[target_col] = source_text\n",
    "    return row\n",
    "\n",
    "# Function to process the Occupation column for both handlebars and brackets\n",
    "def process_occupation_column(row):\n",
    "    # Define the target column for handlebars\n",
    "    handlebars_column = 'Occupation Source URL'\n",
    "\n",
    "    # Process handlebars ({{ }})\n",
    "    if pd.notna(row['Occupation']) and isinstance(row['Occupation'], str):  # Check if the column value is not NaN and is a string\n",
    "        # Extract text between double handlebars\n",
    "        matches = re.findall(r'\\{\\{(.*?)\\}\\}', row['Occupation'])\n",
    "        if matches:\n",
    "            # Remove text between double handlebars and the handlebars themselves\n",
    "            row['Occupation'] = re.sub(r'\\{\\{.*?\\}\\}', '', row['Occupation']).strip()\n",
    "\n",
    "            # Prepare the source text\n",
    "            source_text = '; '.join(matches)\n",
    "\n",
    "            # Append to the target column with a semi-colon separator if it already has data\n",
    "            if pd.notna(row[handlebars_column]) and row[handlebars_column].strip():\n",
    "                row[handlebars_column] += '; ' + source_text\n",
    "            else:\n",
    "                row[handlebars_column] = source_text\n",
    "\n",
    "    # Process brackets ([[ ]])\n",
    "    if pd.notna(row['Occupation']) and isinstance(row['Occupation'], str):  # Check if the column value is not NaN and is a string\n",
    "        # Extract text between double brackets\n",
    "        matches = re.findall(r'\\[\\[(.*?)\\]\\]', row['Occupation'])\n",
    "        if matches:\n",
    "            # Remove text between double brackets and the brackets themselves\n",
    "            row['Occupation'] = re.sub(r'\\[\\[.*?\\]\\]', '', row['Occupation']).strip()\n",
    "\n",
    "            # Prepare the source text\n",
    "            source_text = '; '.join(matches)\n",
    "\n",
    "            # Append to the existing 'Occupation Source' column with a semi-colon separator\n",
    "            if pd.notna(row['Occupation Source']) and row['Occupation Source'].strip():\n",
    "                row['Occupation Source'] += '; ' + source_text\n",
    "            else:\n",
    "                row['Occupation Source'] = source_text\n",
    "    return row\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df_persons.rename(columns={\n",
    "    'Birth Place': 'Place of Birth (P19)',\n",
    "    'Death Place': 'Place of Death',\n",
    "    'Place of Residence': 'Place of Residence'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add new columns to the DataFrame for both handlebars and bracket-based sources\n",
    "df_persons['PoB Source'] = ''\n",
    "df_persons['Place of Death Source'] = ''\n",
    "df_persons['Place of Residence Source'] = ''\n",
    "df_persons['PoB Source URL'] = ''\n",
    "df_persons['Place of Death Source URL'] = ''\n",
    "df_persons['Place of Residence Source URL'] = ''\n",
    "df_persons['Occupation Source URL'] = ''  # Only add this column since 'Occupation Source' already exists\n",
    "\n",
    "# Apply the functions to the DataFrame\n",
    "df_persons = df_persons.apply(process_place_columns, axis=1)\n",
    "df_persons = df_persons.apply(process_occupation_column, axis=1)\n",
    "\n",
    "# Clean up whitespace in all relevant columns\n",
    "columns_to_clean = [\n",
    "    'Place of Birth (P19)', 'Place of Death', 'Place of Residence', 'Occupation',\n",
    "    'PoB Source', 'Place of Death Source', 'Place of Residence Source', 'Occupation Source',\n",
    "    'PoB Source URL', 'Place of Death Source URL', 'Place of Residence Source URL', 'Occupation Source URL'\n",
    "]\n",
    "for col in columns_to_clean:\n",
    "    if col in df_persons.columns:  # Ensure the column exists in the DataFrame\n",
    "        df_persons[col] = df_persons[col].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49 entries, 49 to 1298\n",
      "Data columns (total 52 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   name                           49 non-null     object \n",
      " 1   AltLastName                    24 non-null     object \n",
      " 2   AltMiddleName                  2 non-null      object \n",
      " 3   AltFirstName                   14 non-null     object \n",
      " 4   Maiden Name                    0 non-null      float64\n",
      " 5   Title                          3 non-null      object \n",
      " 6   AssociatedPlaces               5 non-null      object \n",
      " 7   Place of Birth (P19)           9 non-null      object \n",
      " 8   Place of Death                 9 non-null      object \n",
      " 9   Place of Residence             4 non-null      object \n",
      " 10  Source for Places              5 non-null      object \n",
      " 11  Birth Date                     49 non-null     object \n",
      " 12  Death Date                     49 non-null     object \n",
      " 13  Marriage Date                  49 non-null     object \n",
      " 14  Source for Dates               4 non-null      object \n",
      " 15  Spouse                         1 non-null      object \n",
      " 16  Occupation                     15 non-null     object \n",
      " 17  Occupation Source              6 non-null      object \n",
      " 18  Gender                         43 non-null     object \n",
      " 19  Languages                      35 non-null     object \n",
      " 20  Religions                      11 non-null     object \n",
      " 21  Religions Source               1 non-null      object \n",
      " 22  Organizations                  6 non-null      object \n",
      " 23  Organizations Source           0 non-null      float64\n",
      " 24  Associated Letters             7 non-null      float64\n",
      " 25  PRINT ID                       36 non-null     object \n",
      " 26  LOD - WikiData                 16 non-null     object \n",
      " 27  LOD - VIAF                     13 non-null     object \n",
      " 28  LOD - LOC                      11 non-null     object \n",
      " 29  Notes                          9 non-null      object \n",
      " 30  HSP                            21 non-null     float64\n",
      " 31  LSF                            10 non-null     float64\n",
      " 32  SAA                            10 non-null     float64\n",
      " 33  SBB                            0 non-null      float64\n",
      " 34  AFSt                           8 non-null      float64\n",
      " 35  Barclay                        8 non-null      float64\n",
      " 36  Spence                         3 non-null      float64\n",
      " 37  Abraham                        2 non-null      float64\n",
      " 38  Incomplete?                    1 non-null      float64\n",
      " 39  DoB Source                     49 non-null     object \n",
      " 40  DoD (P570) Source              49 non-null     object \n",
      " 41  P26+P2562 Source               49 non-null     object \n",
      " 42  DoB Source URL                 49 non-null     object \n",
      " 43  DoD Source URL                 49 non-null     object \n",
      " 44  P26+P2562 Source URL           49 non-null     object \n",
      " 45  PoB Source                     49 non-null     object \n",
      " 46  Place of Death Source          49 non-null     object \n",
      " 47  Place of Residence Source      49 non-null     object \n",
      " 48  PoB Source URL                 49 non-null     object \n",
      " 49  Place of Death Source URL      49 non-null     object \n",
      " 50  Place of Residence Source URL  49 non-null     object \n",
      " 51  Occupation Source URL          49 non-null     object \n",
      "dtypes: float64(12), object(40)\n",
      "memory usage: 20.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_persons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Anton, Paul', 'Hirschfelde, Germany', ''), ('Schröder, Johann Wilhelm', nan, ''), ('Garside, Abraham', nan, ''), ('Seddon, Thomas', nan, ''), ('Nicholson, Timothy', nan, ''), ('Killam, John', nan, ''), ('Canstein, Carl Hildebrand Von', 'Lindenberg, Germany', ''), ('Orrell, Peter', nan, ''), ('Howgill, Francis', 'Todthorne, Westmorland, England', ''), ('Holbrooke, Joshua', nan, ''), ('Cooper, George', nan, ''), ('Endon, Joseph', nan, ''), ('Griffiths, Francis', nan, ''), ('Hebson, Richard', nan, ''), ('Feddes, Huibert', nan, ''), ('Outerlo, Wouter', nan, ''), ('Herr, Hans', nan, ''), ('Marschall, Wolf Adolph, Erb-Marschall In Thüringen', nan, ''), ('Andriessen, Pieter', nan, ''), ('Hall, Jacob', nan, ''), ('Kintigh, Martin', nan, ''), ('Thornby, Roger', nan, ''), ('Pagiter, William', nan, ''), ('Callenberg, Johann Heinrich', 'Molschleben, Germany', ''), ('Gründler, Johann Ernst', 'Weißensee, Thuringia, Germany', ''), ('Fretwell, Ralph', nan, ''), ('Newton, Samuel', nan, ''), ('Baker, Henry', 'Newtown, Lancashire, England', ''), ('Richardson, John', nan, ''), ('Burnett, John', nan, ''), ('Cook, Thomas', nan, ''), ('Stockdale, William', nan, ''), ('Ward, John', nan, ''), ('Maile, Martin', nan, ''), ('Lange, Joachim', 'Gardelegen, Saxony-Anhalt, Germany', ''), ('Rigge, Ambrose', nan, ''), ('Neufville, David Matheus De', nan, ''), ('Waller, Richard', nan, ''), ('Bennett, Philip', nan, ''), ('Apostool, Andries', 'Amsterdam, Netherlands', ''), ('Osborne, William', nan, ''), ('Milner, Hans Heinrich', nan, ''), ('Meier, Ulrich', nan, ''), ('Gardiner, Daniel', nan, ''), ('Kerfoot, Walter', nan, ''), ('Aldam, Thomas', 'Warmsworth, Yorkshire, England', 'The Journal of George Fox p. 402'), ('Corbett, Thomas', nan, ''), ('Kauffman, David', nan, ''), ('Williams, 1703', nan, '')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(df_persons['name'], df_persons['Place of Birth (P19)'], df_persons['PoB Source'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlleFriezen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boyd's Inhabitants Of London &amp; Family Units 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early Quaker Letters-Nuttall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>England &amp; Wales, Quaker Birth, Marriage, and D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title\n",
       "0                                                   \n",
       "1                                        AlleFriezen\n",
       "2  Boyd's Inhabitants Of London & Family Units 12...\n",
       "3                       Early Quaker Letters-Nuttall\n",
       "4  England & Wales, Quaker Birth, Marriage, and D..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\"Chromium\";v=\"136\", \"Brave\";v=\"136\", \"Not.A/Brand\";v=\"99\"']\n",
      "Bad pipe message: %s [b'sec-ch-ua-mo']\n",
      "Bad pipe message: %s [b'le: ?0\\r\\nsec-ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Win', b'ws NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\\r\\nAcce', b': text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\\r\\nSec-GPC: 1\\r']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Chromium\";v=\"136\", \"Brave\";v=\"136\", \"Not.A/Brand\";v=\"99\"\\r\\nsec-ch-ua-mobile: ?0\\r\\nsec-ch-u']\n",
      "Bad pipe message: %s [b'platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win', b'; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/']\n"
     ]
    }
   ],
   "source": [
    "# RETRIEVES TITLES NEEDED\n",
    "# This code retrieves unique titles from specific columns in the DataFrame and exports them to a CSV file.\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Columns to check for titles\n",
    "cols_to_check = [\n",
    "    'PoB Source',\n",
    "    'Place of Death Source',\n",
    "    'Place of Residence Source',\n",
    "    'Occupation Source',\n",
    "    'DoB Source',\n",
    "    'DoD (P570) Source',\n",
    "    'P26+P2562 Source'\n",
    "]\n",
    "\n",
    "# Initialize an empty set to collect unique titles\n",
    "titles_set = set()\n",
    "\n",
    "# Collect titles from each column\n",
    "for col in cols_to_check:\n",
    "    if col in df_persons.columns:\n",
    "        # Drop NaN values and split entries with semicolons\n",
    "        titles = df_persons[col].dropna().str.split(';').explode().str.strip()\n",
    "        titles_set.update(titles)\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "titles_list = sorted(titles_set)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "titles_df = pd.DataFrame(titles_list, columns=['Title'])\n",
    "\n",
    "# Export the titles to a CSV file\n",
    "titles_df.to_csv('titles_needed.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Display the first few rows of the titles DataFrame\n",
    "titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EEDDKQ-GgkDX",
    "outputId": "3a5fd302-d29d-4fa5-b906-85a985648182"
   },
   "outputs": [],
   "source": [
    "#DON'T RUN THIS FOR NOW\n",
    "import pandas as pd\n",
    "from lccn import get_lccn_for_title, confirm_lccn_matches\n",
    "import time\n",
    "\n",
    "def apply_and_confirm_lccn(df, col, get_lccn_func, confirm_func, delay=1.5, sim_threshold=95, max_retries=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Applies the LCCN retrieval function to a column, confirms the matches, and adds the results to the DataFrame.\n",
    "    \"\"\"\n",
    "    # Step 1: Apply get_lccn_for_title to retrieve LCCNs\n",
    "    lccn_col = f\"{col} LCCN\"\n",
    "    df[lccn_col] = df[col].apply(lambda x: get_lccn_func(x) if pd.notna(x) else [])\n",
    "\n",
    "    # Step 2: Confirm the LCCNs using confirm_lccn_matches\n",
    "    df, confirmed_df = confirm_func(\n",
    "        df=df,\n",
    "        lccn_col=lccn_col,\n",
    "        title_col=col,\n",
    "        delay=delay,\n",
    "        sim_threshold=sim_threshold,\n",
    "        max_retries=max_retries,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Return the updated DataFrame and the confirmed DataFrame\n",
    "    return df, confirmed_df\n",
    "\n",
    "# Columns to check for LCCNs\n",
    "cols_to_check = [\n",
    "    'PoB Source',\n",
    "    'Place of Death Source',\n",
    "    'Place of Residence Source',\n",
    "    'Occupation Source',\n",
    "    'DoB Source',\n",
    "    'DoD (P570) Source',\n",
    "    'P26+P2562 Source'\n",
    "]\n",
    "\n",
    "# Apply and confirm LCCNs for each column\n",
    "for col in cols_to_check:\n",
    "    df_persons, confirmed_lccns = apply_and_confirm_lccn(\n",
    "        df_persons,\n",
    "        col,\n",
    "        get_lccn_for_title,\n",
    "        confirm_lccn_matches,\n",
    "        delay=1.5,  # Delay between requests\n",
    "        sim_threshold=95,\n",
    "        max_retries=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    time.sleep(5)  # Add a delay of 5 seconds between processing each column\n",
    "# Display the updated DataFrame\n",
    "df_persons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vv6uXhJrCPz",
    "outputId": "3f673195-8a5a-4bd6-9c01-d93047179e6a"
   },
   "outputs": [],
   "source": [
    "df_persons.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgAwKHVOcaev"
   },
   "source": [
    "# Convert df_persons columns to template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSaOVufDchbN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca3rkeluWr55"
   },
   "source": [
    "#Deduplicate df_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2-K3hPEgFbS",
    "outputId": "bc1a4e83-676d-4603-b08d-b4f1703cda0e"
   },
   "outputs": [],
   "source": [
    "#fuzzy matching tool\n",
    "%pip install thefuzz\n",
    "from thefuzz import fuzz, process\n",
    "#example--produces similarity score\n",
    "fuzz.ratio('contact zone-encounter', 'contact zones-encounter')\n",
    "\n",
    "#Note: build auto-suggestions for csvs\n",
    "#see process.extract and process.extractOne methods of the fuzz\n",
    "#https://stackoverflow.com/questions/10383044/fuzzy-string-comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uqfh3CRfNivw"
   },
   "outputs": [],
   "source": [
    "# Create a list of all names in the DataFrame\n",
    "names = df_persons['name'].dropna().tolist()\n",
    "\n",
    "# Define a function to find potential duplicates\n",
    "def find_duplicates(name):\n",
    "    if not isinstance(name, str):\n",
    "        return np.nan  # Return NaN if the name is not a string\n",
    "    matches = process.extract(name, names, limit=None, scorer=fuzz.token_sort_ratio)  # Use token_sort_ratio\n",
    "    potential_duplicates = [\n",
    "        match[0] for match in matches if match[1] > 95\n",
    "    ]  # Filter matches with token_sort_ratio > 95\n",
    "\n",
    "    # If no matches with token_sort_ratio, try token_set_ratio\n",
    "    if not potential_duplicates:\n",
    "        matches = process.extract(name, names, limit=None, scorer=fuzz.token_set_ratio)  # Use token_set_ratio\n",
    "        potential_duplicates = [\n",
    "            match[0] for match in matches if match[1] > 95\n",
    "        ]  # Filter matches with token_set_ratio > 95\n",
    "\n",
    "    return potential_duplicates if potential_duplicates else np.nan\n",
    "\n",
    "# Apply the function to create the 'Duplicate(s)?' column\n",
    "df_persons['Duplicate(s)?'] = df_persons['name'].apply(find_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ML6T9EIInVqF",
    "outputId": "26de6b71-f215-4ea1-bab7-c6fbd6700879"
   },
   "outputs": [],
   "source": [
    "df_persons['Duplicate(s)?'].info()\n",
    "df_persons['name'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "CzwX56dMPGpd",
    "outputId": "2e78b8e3-cab4-4ed0-bc4a-5b1fd1b2fa84"
   },
   "outputs": [],
   "source": [
    "df_persons[df_persons['Duplicate(s)?'].notna() & (df_persons['Duplicate(s)?'] != '')][['name', 'Duplicate(s)?']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xl_v9tYGutBd"
   },
   "source": [
    "NOTE: IF THERE'S A DUPE, IT IS DROPPED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apWpJu2SfSKF",
    "outputId": "a222efb1-d284-477a-940c-8400b781e7d1"
   },
   "outputs": [],
   "source": [
    "# cut all rows where there is a dupe? what's the process here?\n",
    "df_persons = df_persons.drop(df_persons.dropna(subset=['Duplicate(s)?']).index)\n",
    "#note what else is in here -- keep only rows where we have data in the columns needed for our LOD schema (need to get the schema)\n",
    "df_persons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY10GRwmztTm"
   },
   "outputs": [],
   "source": [
    "#DONT DO FOR NOW\n",
    "#Drop all rows with wikidata info already\n",
    "#df_persons = df_persons.drop(df_persons.dropna(subset=['LOD - WikiData']).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5o181lr0A3k",
    "outputId": "d3617d6a-6ba0-43b7-cade-11a5699006dd"
   },
   "outputs": [],
   "source": [
    "df_persons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADtAae2YeUPy"
   },
   "outputs": [],
   "source": [
    "#Now that everything is deduplicated, make extra rows for records where columns that may\n",
    "#contain more than one value have 2 or more values. The result will be for the original row to remain, but with the second, third, etc. values\n",
    "#in particular columns missing. New rows will just have the name and the appropriate values.\n",
    "#For example, if a record has two marriage dates, then once fixed it will have the first marriage date and source info in the original row,\n",
    "#and a new row with only the name and the second marriage date and source info.\n",
    "#This is to make working with OpenRefine easier\n",
    "def create_new_rows(df):\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Check for semicolons in 'Marriage Date' and 'Place of Residence'\n",
    "        marriage_dates = str(row['Marriage Date']).split(';')\n",
    "        places_of_residence = str(row['Place of Residence']).split(';')\n",
    "\n",
    "        # Process 'Marriage Date' entries\n",
    "        if len(marriage_dates) > 1:\n",
    "            marriage_date_sources = str(row['P26+P2562 Source']).split(';')\n",
    "            marriage_date_source_urls = str(row['P26+P2562 Source URL']).split(';')\n",
    "\n",
    "            # Keep only the first marriage date in the original row\n",
    "            df.loc[index, 'Marriage Date'] = marriage_dates[0].strip()\n",
    "            df.loc[index, 'P26+P2562 Source'] = marriage_date_sources[0].strip() if marriage_date_sources else ''\n",
    "            df.loc[index, 'P26+P2562 Source URL'] = marriage_date_source_urls[0].strip() if marriage_date_source_urls else ''\n",
    "\n",
    "            # Create new rows for the remaining marriage dates\n",
    "            for i in range(1, len(marriage_dates)):\n",
    "                new_row = pd.Series({'name': row['name']})  # Start with just the name\n",
    "                new_row['Marriage Date'] = marriage_dates[i].strip()\n",
    "                new_row['P26+P2562 Source'] = marriage_date_sources[i].strip() if i < len(marriage_date_sources) else ''\n",
    "                new_row['P26+P2562 Source URL'] = marriage_date_source_urls[i].strip() if i < len(marriage_date_source_urls) else ''\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "        # Process 'Place of Residence' entries (similar logic to Marriage Date)\n",
    "        if len(places_of_residence) > 1:\n",
    "            residence_sources = str(row['Place of Residence Source']).split(';')\n",
    "            residence_source_urls = str(row['Place of Residence Source URL']).split(';')\n",
    "\n",
    "            df.loc[index, 'Place of Residence'] = places_of_residence[0].strip()\n",
    "            df.loc[index, 'Place of Residence Source'] = residence_sources[0].strip() if residence_sources else ''\n",
    "            df.loc[index, 'Place of Residence Source URL'] = residence_source_urls[0].strip() if residence_source_urls else ''\n",
    "\n",
    "            for i in range(1, len(places_of_residence)):\n",
    "                new_row = pd.Series({'name': row['name']})\n",
    "                new_row['Place of Residence'] = places_of_residence[i].strip()\n",
    "                new_row['Place of Residence Source'] = residence_sources[i].strip() if i < len(residence_sources) else ''\n",
    "                new_row['Place of Residence Source URL'] = residence_source_urls[i].strip() if i < len(residence_source_urls) else ''\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "    # Concatenate new rows with the original DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    # Sort the DataFrame by 'name' column\n",
    "    df = df.sort_values(by=['name'], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Apply the function to create new rows\n",
    "df_persons = create_new_rows(df_persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBLUT1HlsiiY"
   },
   "outputs": [],
   "source": [
    "# Export df_person to a CSV file with UTF-8-SIG encoding\n",
    "df_persons_skeletal = df_persons[['name', 'AltLastName', 'AltMiddleName', 'AltFirstName', 'Maiden Name', 'Title', 'Birth Date', 'DoB Source', 'DoB Source URL', 'Death Date', 'DoD (P570) Source', 'DoD Source URL', 'Marriage Date', 'P26+P2562 Source', 'P26+P2562 Source URL', 'Source for Dates', 'Place of Birth (P19)', 'PoB Source', 'PoB Source URL', 'Place of Death', 'Place of Death Source', 'Place of Death Source URL', 'Place of Residence', 'Place of Residence Source', 'Place of Residence Source URL', 'Source for Places', 'Occupation', 'Occupation Source', 'Occupation Source URL', 'Gender', 'LOD - WikiData']]\n",
    "df_persons_skeletal.to_csv('df_persons_skeletal.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Result is a df_persons_skeletal.csv with skeletal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7329dlzZt6B0",
    "outputId": "cdc7088a-2a46-4a5a-f603-646ed878f3b2"
   },
   "outputs": [],
   "source": [
    "df_persons_skeletal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H7Rxlyisjm9"
   },
   "source": [
    "RELATIONSHIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYyIaQtSlold",
    "outputId": "937380d1-f6b2-4014-f18d-8a4bf9e58534"
   },
   "outputs": [],
   "source": [
    "#get authority names for reduced list in df_persons\n",
    "reduced_names = df_persons_skeletal['name'].dropna().tolist()\n",
    "print(reduced_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2qy154ama1k",
    "outputId": "50d5a09e-7235-419a-a4e4-08883d8c1b95"
   },
   "outputs": [],
   "source": [
    "df_relationships.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "94-ruRWTmIr4"
   },
   "outputs": [],
   "source": [
    "#replace all na entries with a comma? for person fields in df_relationships (avoiding nan issue)\n",
    "df_relationships.fillna({'Person 1 Last': ',', 'Person 1 Suffix': ',', 'Person 1 Prefix': ',', 'Person 1 First': ',', 'Person 2 Last': ',', 'Person 2 Suffix': ',', 'Person 2 Prefix': ',', 'Person 2 First': ','}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eFc1XUe7OQ6"
   },
   "outputs": [],
   "source": [
    "#match authority names from df_persons to rows in relationships missing authority names\n",
    "p1_cols = ['Person 1 Last', 'Person 1 Suffix', 'Person 1 Prefix', 'Person 1 First']\n",
    "p2_cols = ['Person 2 Last', 'Person 2 Suffix', 'Person 2 Prefix', 'Person 2 First']\n",
    "\n",
    "#build mock names based on existing information\n",
    "df_relationships['p1_mock_name'] = df_relationships[p1_cols].stack().groupby(level=0).agg(\" \".join)\n",
    "df_relationships['p2_mock_name'] = df_relationships[p2_cols].stack().groupby(level=0).agg(\" \".join)\n",
    "\n",
    "#now, remove commas so match is better\n",
    "df_relationships['p1_mock_name'] = df_relationships['p1_mock_name'].str.replace(',', '', regex=True).str.replace('\\s+', ' ', regex=True)\n",
    "df_relationships['p2_mock_name'] = df_relationships['p2_mock_name'].str.replace(',', '', regex=True).str.replace('\\s+', ' ', regex=True)\n",
    "df_relationships['p1_mock_name'].tail(10)\n",
    "\n",
    "#also, remove extra spaces and trim\n",
    "df_relationships['p1_mock_name'] = df_relationships['p1_mock_name'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "df_relationships['p2_mock_name'] = df_relationships['p2_mock_name'].str.replace('\\s+', ' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TXHEqyKyOkY3",
    "outputId": "71f05170-cce1-4675-9fc7-b565a842dab8"
   },
   "outputs": [],
   "source": [
    "filtered_df_relationships = df_relationships[df_relationships['p1_std_name']==None]\n",
    "df_relationships[['Person 1 Last', 'Person 1 First', 'p1_mock_name']].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XhOHnbYoZcf",
    "outputId": "bcae95a0-2baf-4a04-989f-95e9f99989d5"
   },
   "outputs": [],
   "source": [
    "# Function to find the best match from reduced_names\n",
    "def find_best_match(mock_name, std_name):\n",
    "    if isinstance(std_name, str):  # If std_name already exists, don't replace it\n",
    "        return std_name\n",
    "    if isinstance(mock_name, str):  # If mock_name is valid, find a match\n",
    "        mock_name_cleaned = ' '.join(mock_name.split())  # Normalize whitespace\n",
    "        match = process.extractOne(mock_name_cleaned, reduced_names)\n",
    "        if match and match[1] >= 90:  # Match score >= 90\n",
    "            return match[0]\n",
    "    return \" \"  # Default value if no match is found\n",
    "\n",
    "# Update p1_std_name column\n",
    "df_relationships['p1_std_name'] = df_relationships.apply(\n",
    "    lambda row: find_best_match(row['p1_mock_name'], row['p1_std_name']), axis=1\n",
    ")\n",
    "\n",
    "# Update p2_std_name column\n",
    "df_relationships['p2_std_name'] = df_relationships.apply(\n",
    "    lambda row: find_best_match(row['p2_mock_name'], row['p2_std_name']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ydfn0K8LSRoO",
    "outputId": "59519afb-3e94-4688-d33c-639095985b92"
   },
   "outputs": [],
   "source": [
    "df_relationships[['p1_std_name', 'p2_std_name', 'P1 Relation to P2']].head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8lUq_u2v9Qf"
   },
   "source": [
    "what to do if there aren't standard names for p1 and p2?\n",
    "don't worry about it? what you're doing here is getting all the people data as columns in relationship tab; you are propagating anyways\n",
    "how to approach the bubbling? build dict with keys as all mother-daughter, mother-son, father-daughter, father-son, brother-brother, brother-sister, sister-brother, sister-sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIBmeOQIWl0N"
   },
   "outputs": [],
   "source": [
    "#first dup all rows with terms reversed\n",
    "#then determine all relationships a person has as an array of tuples dict: key: person_name, value: array of tuples (person, relationship)\n",
    "#then go through each person key, get array of tuples; go through array: for each tuple, search for a third party connection with a case statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIhDfWEAdKQP"
   },
   "outputs": [],
   "source": [
    "#OLD NOT IN USE -- HAS USEFUL NESTED CASE/MATCH IN CASE CURRENT SCRIPT IS FAULTY\n",
    "def propagate_relationships_dict(p_dict):\n",
    "  p2_rels = []\n",
    "  p2_people = []\n",
    "  p2_other = []\n",
    "  p1p3_rel=None\n",
    "  p1=''\n",
    "  p2=''\n",
    "  p3=''\n",
    "\n",
    "  # test: script should add ('Francis', 'father') to Bob (and eventually ('John', 'sister-in-law') to Mary)\n",
    "  #has to be run twice I think\n",
    "  p_dict_values = p_dict.values()\n",
    "  print(p_dict_values)\n",
    "  #add people who are not in the keys but in the values with just 'self' as a relation\n",
    "  p_dict_new = [x[0] for x in p_dict_values if p_dict.get(x[0]) is None]\n",
    "  for new_p in p_dict_new:\n",
    "    p_dict[new_p] = [(new_p, 'self')]\n",
    "  #print(p_dict)\n",
    "  #loop through twice to sweep in and process added relationships\n",
    "  for _ in range(2):\n",
    "    for key, value in p_dict.items():\n",
    "      p1=key\n",
    "      for p2_tuple in value:\n",
    "        p2=p2_tuple[0]\n",
    "        p1p2_rel=p2_tuple[1]\n",
    "        if (p_dict.get(p2) is not None) and (p1 != p2):\n",
    "          p2p3_rels = [x[1] for i, x in enumerate(p_dict.get(p2))]\n",
    "          p2p3_people =  [x[0] for i, x in enumerate(p_dict.get(p2))]\n",
    "          for i, p2p3_rel in enumerate(p2p3_rels):\n",
    "              #print(p1p2_rel, p2, p2p3_rel, p2p3_people[i])\n",
    "              match p1p2_rel:\n",
    "                case 'mother':\n",
    "                  match p2p3_rel:\n",
    "                    case 'mother'|'father':\n",
    "                      p1p3_rel = 'grandmother'\n",
    "                    case 'sister'|'brother':\n",
    "                      p1p3_rel = 'mother'\n",
    "                    case 'daughter'|'son':\n",
    "                      p1p3_rel = 'wife'\n",
    "                case 'father':\n",
    "                  match p2p3_rel:\n",
    "                    case 'mother'|'father':\n",
    "                      p1p3_rel = 'grandfather'\n",
    "                    case 'sister'|'brother':\n",
    "                      p1p3_rel = 'father'\n",
    "                    case 'daughter'|'son':\n",
    "                      p1p3_rel = 'husband'\n",
    "                case 'husband':\n",
    "                  match p2p3_rel:\n",
    "                    case 'mother':\n",
    "                      p1p3_rel = 'father'\n",
    "                    case 'sister'|'brother':\n",
    "                      p1p3_rel = 'brother-in-law'\n",
    "                    case 'daughter':\n",
    "                      p1p3_rel = 'son-in-law'\n",
    "                case 'wife':\n",
    "                  match p2p3_rel:\n",
    "                    case 'father':\n",
    "                      p1p3_rel = 'mother'\n",
    "                    case 'sister'|'brother':\n",
    "                      p1p3_rel = 'sister-in-law'\n",
    "                    case 'son':\n",
    "                      p1p3_rel = 'daughter-in-law'\n",
    "                case 'sister':\n",
    "                  match p2p3_rel:\n",
    "                    case 'brother'|'sister':\n",
    "                      p1p3_rel='sister'\n",
    "                    case 'father'|'mother':\n",
    "                      p1p3_rel='aunt'\n",
    "                    case 'son'|'daughter':\n",
    "                      p1p3_rel='daughter'\n",
    "                case 'brother':\n",
    "                  match p2p3_rel:\n",
    "                    case 'brother'|'sister':\n",
    "                      p1p3_rel='brother'\n",
    "                    case 'father'|'mother':\n",
    "                      p1p3_rel='uncle'\n",
    "                    case 'son'|'daughter':\n",
    "                      p1p3_rel='son'\n",
    "                case 'daughter':\n",
    "                  match p2p3_rel:\n",
    "                    case 'son'|'daughter':\n",
    "                      p1p3_rel='granddaughter'\n",
    "                    case 'sister'|'brother':\n",
    "                      p1p3_rel='niece'\n",
    "                    case 'mother'|'father':\n",
    "                      p1p3_rel='sister'\n",
    "                case 'son':\n",
    "                  match p2p3_rel:\n",
    "                    case 'son'|'daughter':\n",
    "                      p1p3_rel='grandson'\n",
    "                    case 'sister'|'brother':\n",
    "                      p1p3_rel='nephew'\n",
    "                    case 'mother'|'father':\n",
    "                      p1p3_rel='brother'\n",
    "              if p1p3_rel is not None:\n",
    "                p_dict[p1].append((p2p3_people[i], p1p3_rel))\n",
    "              p1p3_rel = None\n",
    "          #do something\n",
    "  for key, value in p_dict.items():\n",
    "    p_dict[key]=list(set(value))\n",
    "  #print(p_dict)\n",
    "        #case 'wife':\n",
    "        # print('here')\n",
    "          #do something\n",
    "        #case 'brother':\n",
    "        #case 'sister':\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR0hToTUIEtB"
   },
   "outputs": [],
   "source": [
    "p_dict = {'Bob': [('Mary', 'husband'), ('John', 'brother')], 'Mary': [('Bob', 'wife'), ('Francis', 'mother')], 'Francis': [('George', 'sister'), ('Albert', 'mother')]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCUrKvYlIMDm",
    "outputId": "0105b675-467e-43dc-e091-542f2fd78271"
   },
   "outputs": [],
   "source": [
    "propagate_relationships_dict(p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py44K7vgIEP5"
   },
   "outputs": [],
   "source": [
    "def propagate_relationships(df_relationships):\n",
    "    \"\"\"\n",
    "    Propagates relationships in the DataFrame by:\n",
    "    1. Adding missing reciprocal relationships.\n",
    "    2. Inferring new relationships based on existing ones.\n",
    "    \"\"\"\n",
    "    # Create a list to store new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Step 1: Add missing reciprocal relationships\n",
    "    for _, row in df_relationships.iterrows():\n",
    "        p1, p2, p1_to_p2, p2_to_p1 = row['p1_std_name'], row['p2_std_name'], row['P1 Relation to P2'], row['P2 Relation to P1']\n",
    "\n",
    "        # Add reciprocal relationship for P1 -> P2\n",
    "        if not ((df_relationships['p1_std_name'] == p2) &\n",
    "                (df_relationships['p2_std_name'] == p1) &\n",
    "                (df_relationships['P1 Relation to P2'] == p2_to_p1)).any():\n",
    "            new_rows.append({'p1_std_name': p2, 'p2_std_name': p1, 'P1 Relation to P2': p2_to_p1, 'P2 Relation to P1': p1_to_p2})\n",
    "\n",
    "    # Step 2: Infer relationships involving a third person\n",
    "    for _, row1 in df_relationships.iterrows():\n",
    "        p1, p2, p1_to_p2 = row1['p1_std_name'], row1['p2_std_name'], row1['P1 Relation to P2']\n",
    "        for _, row2 in df_relationships.iterrows():\n",
    "            if row1['p2_std_name'] == row2['p1_std_name']:  # Check if P2 in row1 is P1 in row2\n",
    "                p3, p2_to_p3 = row2['p2_std_name'], row2['P1 Relation to P2']\n",
    "\n",
    "                # Infer relationship between P1 and P3\n",
    "                inferred_relation = None\n",
    "                reverse_relation = None\n",
    "                match p1_to_p2:\n",
    "                    case 'mother':\n",
    "                        match p2_to_p3:\n",
    "                            case 'mother' | 'father':\n",
    "                                inferred_relation, reverse_relation = 'grandmother', 'granddaughter'\n",
    "                            case 'son' | 'daughter':\n",
    "                                inferred_relation, reverse_relation = 'mother', 'child'\n",
    "                    case 'father':\n",
    "                        match p2_to_p3:\n",
    "                            case 'mother' | 'father':\n",
    "                                inferred_relation, reverse_relation = 'grandfather', 'grandson'\n",
    "                            case 'son' | 'daughter':\n",
    "                                inferred_relation, reverse_relation = 'father', 'child'\n",
    "                    case 'husband':\n",
    "                        match p2_to_p3:\n",
    "                            case 'mother':\n",
    "                                inferred_relation, reverse_relation = 'father', 'son'\n",
    "                            case 'daughter':\n",
    "                                inferred_relation, reverse_relation = 'son-in-law', 'father-in-law'\n",
    "                    case 'wife':\n",
    "                        match p2_to_p3:\n",
    "                            case 'father':\n",
    "                                inferred_relation, reverse_relation = 'mother', 'daughter'\n",
    "                            case 'son':\n",
    "                                inferred_relation, reverse_relation = 'daughter-in-law', 'mother-in-law'\n",
    "                    case 'brother':\n",
    "                        match p2_to_p3:\n",
    "                            case 'brother' | 'sister':\n",
    "                                inferred_relation, reverse_relation = 'brother', 'brother'\n",
    "                            case 'son' | 'daughter':\n",
    "                                inferred_relation, reverse_relation = 'uncle', 'nephew'\n",
    "                    case 'sister':\n",
    "                        match p2_to_p3:\n",
    "                            case 'brother' | 'sister':\n",
    "                                inferred_relation, reverse_relation = 'sister', 'sister'\n",
    "                            case 'son' | 'daughter':\n",
    "                                inferred_relation, reverse_relation = 'aunt', 'niece'\n",
    "\n",
    "                # Add inferred relationship if applicable\n",
    "                if inferred_relation:\n",
    "                    if not ((df_relationships['p1_std_name'] == p1) &\n",
    "                            (df_relationships['p2_std_name'] == p3) &\n",
    "                            (df_relationships['P1 Relation to P2'] == inferred_relation)).any():\n",
    "                        new_rows.append({'p1_std_name': p1, 'p2_std_name': p3, 'P1 Relation to P2': inferred_relation, 'P2 Relation to P1': reverse_relation})\n",
    "\n",
    "                    # Add the reverse relationship\n",
    "                    if not ((df_relationships['p1_std_name'] == p3) &\n",
    "                            (df_relationships['p2_std_name'] == p1) &\n",
    "                            (df_relationships['P1 Relation to P2'] == reverse_relation)).any():\n",
    "                        new_rows.append({'p1_std_name': p3, 'p2_std_name': p1, 'P1 Relation to P2': reverse_relation, 'P2 Relation to P1': inferred_relation})\n",
    "\n",
    "    # Step 3: Add new rows to the DataFrame\n",
    "    if new_rows:\n",
    "        df_relationships = pd.concat([df_relationships, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    print(new_rows)\n",
    "    return df_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "8FrM0-vhIP-1",
    "outputId": "f7c1fba6-bc9d-42a3-e7bd-5f37520ee19d"
   },
   "outputs": [],
   "source": [
    "propagate_relationships(df_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8ybL-B0pxuB"
   },
   "outputs": [],
   "source": [
    "# Remove rows where p1_std_name or p2_std_name do not contain any alphanumeric characters\n",
    "df_relationships = df_relationships[\n",
    "    df_relationships['p1_std_name'].str.contains(r'[A-Za-z0-9]', na=False) &\n",
    "    df_relationships['p2_std_name'].str.contains(r'[A-Za-z0-9]', na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "mYsnm211pzwv",
    "outputId": "21bb5c7d-7218-408d-86dc-ea9fe7cc8030"
   },
   "outputs": [],
   "source": [
    "df_relationships.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRaugD1Bqvu6"
   },
   "outputs": [],
   "source": [
    "# Export relationships as triples\n",
    "# Create a new DataFrame with the desired columns\n",
    "df_triples = df_relationships[['p1_std_name', 'P1 Relation to P2', 'p2_std_name']].rename(\n",
    "    columns={'p1_std_name': 'Person1', 'P1 Relation to P2': 'Relationship', 'p2_std_name': 'Person2'}\n",
    ")\n",
    "\n",
    "# Export the DataFrame to a spreadsheet with UTF-8-SIG encoding\n",
    "df_triples.to_csv('df_relationships_triples.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
